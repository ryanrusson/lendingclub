{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Read in the data from the .pkl\n",
    "df = pd.read_pickle(\"./data/fe_data.pkl\")\n",
    "\n",
    "num_cols = list(df._get_numeric_data().columns)\n",
    "cat_cols = list(set(df.columns) - set(df._get_numeric_data().columns))\n",
    "target = ['is_bad']\n",
    "num_cols.remove('is_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip_code               object\n",
       "inq_last_6mths_bin     object\n",
       "delinq_2yrs_bin        object\n",
       "policy_code            object\n",
       "pymnt_plan             object\n",
       "addr_state             object\n",
       "home_ownership         object\n",
       "initial_list_status    object\n",
       "verification_status    object\n",
       "purpose_cat            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[cat_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_length                       int64\n",
       "annual_inc                     float64\n",
       "debt_to_income                 float64\n",
       "delinq_2yrs                    float64\n",
       "inq_last_6mths                 float64\n",
       "mths_since_last_delinq         float64\n",
       "mths_since_last_record           int64\n",
       "open_acc                       float64\n",
       "pub_rec                          int64\n",
       "revol_bal                        int64\n",
       "revol_util                     float64\n",
       "total_acc                      float64\n",
       "mths_since_last_major_derog      int64\n",
       "cr_line_yrs                    float64\n",
       "cr_line_mths                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[num_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_bad    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   is_bad                       10000 non-null  int64  \n",
      " 1   emp_length                   10000 non-null  int64  \n",
      " 2   home_ownership               10000 non-null  object \n",
      " 3   annual_inc                   9999 non-null   float64\n",
      " 4   verification_status          10000 non-null  object \n",
      " 5   pymnt_plan                   10000 non-null  object \n",
      " 6   purpose_cat                  10000 non-null  object \n",
      " 7   zip_code                     10000 non-null  object \n",
      " 8   addr_state                   10000 non-null  object \n",
      " 9   debt_to_income               10000 non-null  float64\n",
      " 10  delinq_2yrs                  10000 non-null  float64\n",
      " 11  inq_last_6mths               10000 non-null  float64\n",
      " 12  mths_since_last_delinq       10000 non-null  float64\n",
      " 13  mths_since_last_record       10000 non-null  int64  \n",
      " 14  open_acc                     9995 non-null   float64\n",
      " 15  pub_rec                      10000 non-null  int64  \n",
      " 16  revol_bal                    10000 non-null  int64  \n",
      " 17  revol_util                   9974 non-null   float64\n",
      " 18  total_acc                    9995 non-null   float64\n",
      " 19  initial_list_status          10000 non-null  object \n",
      " 20  mths_since_last_major_derog  10000 non-null  int64  \n",
      " 21  policy_code                  10000 non-null  object \n",
      " 22  cr_line_yrs                  9995 non-null   float64\n",
      " 23  cr_line_mths                 10000 non-null  int64  \n",
      " 24  delinq_2yrs_bin              10000 non-null  object \n",
      " 25  inq_last_6mths_bin           10000 non-null  object \n",
      "dtypes: float64(9), int64(7), object(10)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute more of the missing data\n",
    "df['annual_inc'].fillna(df.annual_inc.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['open_acc'].fillna(df.open_acc.mode()[0], inplace=True)\n",
    "df['open_acc'] = df['open_acc'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['revol_util'].fillna(df.revol_util.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_acc'].fillna(df.total_acc.median(), inplace=True)\n",
    "df['total_acc'] = df['total_acc'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cr_line_yrs'].fillna(df.cr_line_yrs.median(), inplace=True)\n",
    "df['cr_line_yrs'] = df['cr_line_yrs'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_region(x):\n",
    "    return x[1:3]\n",
    "\n",
    "df['zip_region'] = df.zip_code.apply(zip_region)\n",
    "df.drop('zip_code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   is_bad                       10000 non-null  int64  \n",
      " 1   emp_length                   10000 non-null  int64  \n",
      " 2   home_ownership               10000 non-null  object \n",
      " 3   annual_inc                   10000 non-null  float64\n",
      " 4   verification_status          10000 non-null  object \n",
      " 5   pymnt_plan                   10000 non-null  object \n",
      " 6   purpose_cat                  10000 non-null  object \n",
      " 7   addr_state                   10000 non-null  object \n",
      " 8   debt_to_income               10000 non-null  float64\n",
      " 9   delinq_2yrs                  10000 non-null  float64\n",
      " 10  inq_last_6mths               10000 non-null  float64\n",
      " 11  mths_since_last_delinq       10000 non-null  float64\n",
      " 12  mths_since_last_record       10000 non-null  int64  \n",
      " 13  open_acc                     10000 non-null  int64  \n",
      " 14  pub_rec                      10000 non-null  int64  \n",
      " 15  revol_bal                    10000 non-null  int64  \n",
      " 16  revol_util                   10000 non-null  float64\n",
      " 17  total_acc                    10000 non-null  int64  \n",
      " 18  initial_list_status          10000 non-null  object \n",
      " 19  mths_since_last_major_derog  10000 non-null  int64  \n",
      " 20  policy_code                  10000 non-null  object \n",
      " 21  cr_line_yrs                  10000 non-null  int64  \n",
      " 22  cr_line_mths                 10000 non-null  int64  \n",
      " 23  delinq_2yrs_bin              10000 non-null  object \n",
      " 24  inq_last_6mths_bin           10000 non-null  object \n",
      " 25  zip_region                   10000 non-null  object \n",
      "dtypes: float64(6), int64(10), object(10)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encode and One-Hot Encode Categorical Variables\n",
    "- NOTE: I may need to reduce the dimensions after the one-hot encoding, but we will see how this goes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymnt_plan\n",
      "initial_list_status\n",
      "delinq_2yrs_bin\n",
      "inq_last_6mths_bin\n",
      "4 columns were label encoded.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "count = 0\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:     \n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            count += 1\n",
    "            print (col)\n",
    "            \n",
    "print('%d columns were label encoded.' % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 210)\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the data with basic feature engineering\n",
    "df.to_pickle(\"./data/explore_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(df._get_numeric_data().columns)\n",
    "cat_cols = list(set(df.columns) - set(df._get_numeric_data().columns))\n",
    "target = ['is_bad']\n",
    "num_cols.remove('is_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 20\n",
    "\n",
    "# Try just using a train/test split at first without sorting the values by time (I don't know how truly time-based this model will be)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('is_bad',axis=1), df['is_bad'], test_size=0.10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to try and balance the classes within the training set\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=seed)\n",
    "x_train_r, y_train_r = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15684"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, random_state=20)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with simple logistic regression classifier for second baseline attempt\n",
    "clf_lr = LogisticRegression(C = 0.0001, random_state=seed)\n",
    "clf_lr.fit(x_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC score: 0.5815987346804138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.56      0.69       863\n",
      "           1       0.18      0.61      0.28       137\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.54      0.58      0.48      1000\n",
      "weighted avg       0.80      0.56      0.63      1000\n",
      "\n",
      "Min accuracy to beat for just random guessing in the TEST set:\n",
      "0.863\n"
     ]
    }
   ],
   "source": [
    "ypred = clf_lr.predict(X_test)\n",
    "score = roc_auc_score(y_test, ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(classification_report(y_test, ypred))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(y_test[y_test == 0])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=20)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "clf_rf.fit(x_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC score: 0.5134397915944211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       863\n",
      "           1       0.67      0.03      0.06       137\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.77      0.51      0.49      1000\n",
      "weighted avg       0.84      0.86      0.81      1000\n",
      "\n",
      "Min accuracy to beat for just random guessing in the TEST set:\n",
      "0.863\n"
     ]
    }
   ],
   "source": [
    "ypred = clf_rf.predict(X_test)\n",
    "score = roc_auc_score(y_test, ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(classification_report(y_test, ypred))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(y_test[y_test == 0])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "dtrain = lgb.Dataset(x_train_r, label=y_train_r)\n",
    "dvalid = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC score: 0.5846943694970017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.94       863\n",
      "           1       0.83      0.18      0.29       137\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.86      0.58      0.61      1000\n",
      "weighted avg       0.88      0.88      0.85      1000\n",
      "\n",
      "Min accuracy to beat for just random guessing in the TEST set:\n",
      "0.856\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "ypred = bst.predict(X_test)\n",
    "ypred_thresh = [1 if x >= 0.5 else 0 for x in ypred]\n",
    "\n",
    "score = metrics.roc_auc_score(y_test, ypred_thresh)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(metrics.classification_report(y_test, ypred_thresh))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(test[test.is_bad == 0])/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Methods + Kfold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "clf_lgb = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "n_scores = cross_val_score(clf_lgb, x_train_r, y_train_r, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#clf_lgb.fit(x_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 1569  1570  1571 ... 15681 15682 15683] | test: [   0    1    2 ... 1566 1567 1568]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [1569 1570 1571 ... 3135 3136 3137]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [3138 3139 3140 ... 4704 4705 4706]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [4707 4708 4709 ... 6273 6274 6275]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [6276 6277 6278 ... 7841 7842 7843]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [7844 7845 7846 ... 9409 9410 9411]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [ 9412  9413  9414 ... 10977 10978 10979]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [10980 10981 10982 ... 12545 12546 12547]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [12548 12549 12550 ... 14113 14114 14115]\n",
      "Train: [    0     1     2 ... 14113 14114 14115] | test: [14116 14117 14118 ... 15681 15682 15683]\n"
     ]
    }
   ],
   "source": [
    "for train_indices, test_indices in kfold.split(x_train_r):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93499044, 0.94582537, 0.93945188, 0.9292543 , 0.93813776,\n",
       "       0.9375    , 0.93686224, 0.9317602 , 0.93239796, 0.92538265,\n",
       "       0.94518802, 0.93881453, 0.93116635, 0.9292543 , 0.93112245,\n",
       "       0.9375    , 0.93558673, 0.93558673, 0.94132653, 0.92091837,\n",
       "       0.93116635, 0.93116635, 0.93371574, 0.94072658, 0.93813776,\n",
       "       0.94706633, 0.93239796, 0.92283163, 0.93622449, 0.93558673])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call `fit` before exploiting the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-f8118466d9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#score = metrics.roc_auc_score(validation['is_bad'], ypred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         result = self.predict_proba(X, raw_score, num_iteration,\n\u001b[0;32m--> 814\u001b[0;31m                                     pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \"\"\"\n\u001b[1;32m    862\u001b[0m         result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,\n\u001b[0;32m--> 863\u001b[0;31m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \"\"\"\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLGBMNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estimator not fitted, call `fit` before exploiting the model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMCheckArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Estimator not fitted, call `fit` before exploiting the model."
     ]
    }
   ],
   "source": [
    "#ypred = clf_lgb.predict(validation[feature_cols])\n",
    "#score = metrics.roc_auc_score(validation['is_bad'], ypred)\n",
    "\n",
    "ypred = clf_lgb.predict(X_test)\n",
    "score = metrics.roc_auc_score(y_test, ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(metrics.classification_report(y_test, ypred))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(validation[validation.is_bad == 0])/len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
