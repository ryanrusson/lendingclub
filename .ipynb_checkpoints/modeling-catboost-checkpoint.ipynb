{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Read in the data from the .pkl\n",
    "df = pd.read_pickle(\"./data/baseline_fe_data.pkl\")\n",
    "\n",
    "num_cols = list(df._get_numeric_data().columns)\n",
    "cat_cols = list(set(df.columns) - set(df._get_numeric_data().columns))\n",
    "target = ['is_bad']\n",
    "num_cols.remove('is_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verification_status    object\n",
       "policy_code            object\n",
       "purpose_cat            object\n",
       "initial_list_status    object\n",
       "pymnt_plan             object\n",
       "zip_code               object\n",
       "addr_state             object\n",
       "home_ownership         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[cat_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emp_length                       int64\n",
       "annual_inc                     float64\n",
       "debt_to_income                 float64\n",
       "delinq_2yrs                    float64\n",
       "inq_last_6mths                 float64\n",
       "mths_since_last_delinq         float64\n",
       "mths_since_last_record           int64\n",
       "open_acc                       float64\n",
       "pub_rec                          int64\n",
       "revol_bal                        int64\n",
       "revol_util                     float64\n",
       "total_acc                      float64\n",
       "mths_since_last_major_derog      int64\n",
       "cr_line_yrs                    float64\n",
       "cr_line_mths                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[num_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_bad    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   is_bad                       10000 non-null  int64  \n",
      " 1   emp_length                   10000 non-null  int64  \n",
      " 2   home_ownership               10000 non-null  object \n",
      " 3   annual_inc                   10000 non-null  float64\n",
      " 4   verification_status          10000 non-null  object \n",
      " 5   pymnt_plan                   10000 non-null  object \n",
      " 6   purpose_cat                  10000 non-null  object \n",
      " 7   zip_code                     10000 non-null  object \n",
      " 8   addr_state                   10000 non-null  object \n",
      " 9   debt_to_income               10000 non-null  float64\n",
      " 10  delinq_2yrs                  10000 non-null  float64\n",
      " 11  inq_last_6mths               10000 non-null  float64\n",
      " 12  mths_since_last_delinq       10000 non-null  float64\n",
      " 13  mths_since_last_record       10000 non-null  int64  \n",
      " 14  open_acc                     10000 non-null  float64\n",
      " 15  pub_rec                      10000 non-null  int64  \n",
      " 16  revol_bal                    10000 non-null  int64  \n",
      " 17  revol_util                   10000 non-null  float64\n",
      " 18  total_acc                    10000 non-null  float64\n",
      " 19  initial_list_status          10000 non-null  object \n",
      " 20  mths_since_last_major_derog  10000 non-null  int64  \n",
      " 21  policy_code                  10000 non-null  object \n",
      " 22  cr_line_yrs                  10000 non-null  float64\n",
      " 23  cr_line_mths                 10000 non-null  int64  \n",
      "dtypes: float64(9), int64(7), object(8)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def zip_region(x):\n",
    "#    return x[1:3]\n",
    "#\n",
    "#df['zip_region'] = df.zip_code.apply(zip_region)\n",
    "df.drop('zip_code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   is_bad                       10000 non-null  int64  \n",
      " 1   emp_length                   10000 non-null  int64  \n",
      " 2   home_ownership               10000 non-null  object \n",
      " 3   annual_inc                   10000 non-null  float64\n",
      " 4   verification_status          10000 non-null  object \n",
      " 5   pymnt_plan                   10000 non-null  object \n",
      " 6   purpose_cat                  10000 non-null  object \n",
      " 7   addr_state                   10000 non-null  object \n",
      " 8   debt_to_income               10000 non-null  float64\n",
      " 9   delinq_2yrs                  10000 non-null  float64\n",
      " 10  inq_last_6mths               10000 non-null  float64\n",
      " 11  mths_since_last_delinq       10000 non-null  float64\n",
      " 12  mths_since_last_record       10000 non-null  int64  \n",
      " 13  open_acc                     10000 non-null  float64\n",
      " 14  pub_rec                      10000 non-null  int64  \n",
      " 15  revol_bal                    10000 non-null  int64  \n",
      " 16  revol_util                   10000 non-null  float64\n",
      " 17  total_acc                    10000 non-null  float64\n",
      " 18  initial_list_status          10000 non-null  object \n",
      " 19  mths_since_last_major_derog  10000 non-null  int64  \n",
      " 20  policy_code                  10000 non-null  object \n",
      " 21  cr_line_yrs                  10000 non-null  float64\n",
      " 22  cr_line_mths                 10000 non-null  int64  \n",
      "dtypes: float64(9), int64(7), object(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encode and One-Hot Encode Categorical Variables\n",
    "- NOTE: I may need to reduce the dimensions after the one-hot encoding, but we will see how this goes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymnt_plan\n",
      "initial_list_status\n",
      "2 columns were label encoded.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "count = 0\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:     \n",
    "            le = preprocessing.LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            count += 1\n",
    "            print (col)\n",
    "            \n",
    "print('%d columns were label encoded.' % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 108)\n"
     ]
    }
   ],
   "source": [
    "df = pd.get_dummies(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the data with basic feature engineering\n",
    "df.to_pickle(\"./data/non_standard_explore_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Distribution of is_bad:\n",
      "0.1285\n",
      "\n",
      "Test Distribution of is_bad:\n",
      "0.1335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 20\n",
    "\n",
    "# Try just using a train/test split at first without sorting the values by time (I don't know how truly time-based this model will be)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('is_bad',axis=1), df['is_bad'], test_size=0.20, random_state=seed)\n",
    "\n",
    "print(\"Training Distribution of is_bad:\")\n",
    "print(len(y_train[y_train==1])/len(y_train))\n",
    "\n",
    "print()\n",
    "print(\"Test Distribution of is_bad:\")\n",
    "print(len(y_test[y_test==1])/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scale the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Correlations with \"is_bad\"\n",
      "--------------------\n",
      "purpose_cat_major purchase small business        0.057989\n",
      "purpose_cat_educational small business           0.057989\n",
      "purpose_cat_small business                       0.079963\n",
      "purpose_cat_home improvement small business      0.086037\n",
      "revol_util                                       0.087797\n",
      "purpose_cat_small business small business        0.089867\n",
      "purpose_cat_credit card small business           0.106990\n",
      "purpose_cat_other small business                 0.110097\n",
      "purpose_cat_debt consolidation small business    0.263194\n",
      "is_bad                                           1.000000\n",
      "Name: is_bad, dtype: float64\n",
      "\n",
      "Most Negative Correlations with \"is_bad\"\n",
      "--------------------\n",
      "total_acc                          -0.055271\n",
      "purpose_cat_credit card            -0.055271\n",
      "annual_inc                         -0.050966\n",
      "verification_status_not verified   -0.050158\n",
      "cr_line_mths                       -0.035356\n",
      "purpose_cat_wedding                -0.035054\n",
      "emp_length                         -0.033449\n",
      "purpose_cat_debt consolidation     -0.032835\n",
      "home_ownership_mortgage            -0.031544\n",
      "purpose_cat_car                    -0.026283\n",
      "Name: is_bad, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# See how much this boosts the correlations\n",
    "corr = df.corr()['is_bad'].sort_values()\n",
    "\n",
    "# Display correlations\n",
    "print('Most Positive Correlations with \"is_bad\"')\n",
    "print(20*\"-\")\n",
    "print(corr.tail(10))\n",
    "print()\n",
    "print('Most Negative Correlations with \"is_bad\"')\n",
    "print(20*\"-\")\n",
    "print(corr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(df._get_numeric_data().columns)\n",
    "cat_cols = list(set(df.columns) - set(df._get_numeric_data().columns))\n",
    "target = ['is_bad']\n",
    "num_cols.remove('is_bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to try and balance the classes within the training set\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=seed)\n",
    "x_train_r, y_train_r = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 3 initial models: LR Classifier, RF Classifier, LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, random_state=20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with simple logistic regression classifier for second baseline attempt\n",
    "clf_lr = LogisticRegression(C = 0.001, random_state=seed)\n",
    "clf_lr.fit(x_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC score: 0.6321548439522725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.68      0.78      1733\n",
      "           1       0.22      0.58      0.32       267\n",
      "\n",
      "    accuracy                           0.67      2000\n",
      "   macro avg       0.57      0.63      0.55      2000\n",
      "weighted avg       0.82      0.67      0.72      2000\n",
      "\n",
      "Min accuracy to beat for just random guessing in the TEST set:\n",
      "0.8665\n"
     ]
    }
   ],
   "source": [
    "ypred = clf_lr.predict(X_test)\n",
    "score = roc_auc_score(y_test, ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(classification_report(y_test, ypred))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(y_test[y_test == 0])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_leaf=2, min_samples_split=4,\n",
       "                       n_estimators=200, oob_score=True, random_state=20)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=200, \n",
    "                                random_state=seed,\n",
    "                                min_samples_split=4,\n",
    "                                min_samples_leaf=2,\n",
    "                                oob_score=True)\n",
    "clf_rf.fit(x_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC score: 0.5558912582583946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      1733\n",
      "           1       0.97      0.11      0.20       267\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.92      0.56      0.57      2000\n",
      "weighted avg       0.89      0.88      0.84      2000\n",
      "\n",
      "Min accuracy to beat for just random guessing in the TEST set:\n",
      "0.8665\n"
     ]
    }
   ],
   "source": [
    "ypred = clf_rf.predict(X_test)\n",
    "score = roc_auc_score(y_test, ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(classification_report(y_test, ypred))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(y_test[y_test == 0])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "dtrain = lgb.Dataset(x_train_r, label=y_train_r)\n",
    "dvalid = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC score: 0.5900291974904422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      1733\n",
      "           1       0.89      0.18      0.30       267\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.89      0.59      0.62      2000\n",
      "weighted avg       0.89      0.89      0.85      2000\n",
      "\n",
      "Min accuracy to beat for just random guessing in the TEST set:\n",
      "0.8665\n"
     ]
    }
   ],
   "source": [
    "ypred = bst.predict(X_test)\n",
    "ypred_thresh = [1 if x >= 0.5 else 0 for x in ypred]\n",
    "\n",
    "score = roc_auc_score(y_test, ypred_thresh)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(classification_report(y_test, ypred_thresh))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(y_test[y_test == 0])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Methods + Kfold Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "clf_lgb = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "n_scores = cross_val_score(clf_lgb, x_train_r, y_train_r, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#clf_lgb.fit(x_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 1569  1570  1571 ... 15681 15682 15683] | test: [   0    1    2 ... 1566 1567 1568]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [1569 1570 1571 ... 3135 3136 3137]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [3138 3139 3140 ... 4704 4705 4706]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [4707 4708 4709 ... 6273 6274 6275]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [6276 6277 6278 ... 7841 7842 7843]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [7844 7845 7846 ... 9409 9410 9411]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [ 9412  9413  9414 ... 10977 10978 10979]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [10980 10981 10982 ... 12545 12546 12547]\n",
      "Train: [    0     1     2 ... 15681 15682 15683] | test: [12548 12549 12550 ... 14113 14114 14115]\n",
      "Train: [    0     1     2 ... 14113 14114 14115] | test: [14116 14117 14118 ... 15681 15682 15683]\n"
     ]
    }
   ],
   "source": [
    "for train_indices, test_indices in kfold.split(x_train_r):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93499044, 0.94582537, 0.93945188, 0.9292543 , 0.93813776,\n",
       "       0.9375    , 0.93686224, 0.9317602 , 0.93239796, 0.92538265,\n",
       "       0.94518802, 0.93881453, 0.93116635, 0.9292543 , 0.93112245,\n",
       "       0.9375    , 0.93558673, 0.93558673, 0.94132653, 0.92091837,\n",
       "       0.93116635, 0.93116635, 0.93371574, 0.94072658, 0.93813776,\n",
       "       0.94706633, 0.93239796, 0.92283163, 0.93622449, 0.93558673])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call `fit` before exploiting the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-f8118466d9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#score = metrics.roc_auc_score(validation['is_bad'], ypred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         result = self.predict_proba(X, raw_score, num_iteration,\n\u001b[0;32m--> 814\u001b[0;31m                                     pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \"\"\"\n\u001b[1;32m    862\u001b[0m         result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,\n\u001b[0;32m--> 863\u001b[0;31m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \"\"\"\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLGBMNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estimator not fitted, call `fit` before exploiting the model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMCheckArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Estimator not fitted, call `fit` before exploiting the model."
     ]
    }
   ],
   "source": [
    "#ypred = clf_lgb.predict(validation[feature_cols])\n",
    "#score = metrics.roc_auc_score(validation['is_bad'], ypred)\n",
    "\n",
    "ypred = clf_lgb.predict(X_test)\n",
    "score = metrics.roc_auc_score(y_test, ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")\n",
    "print(metrics.classification_report(y_test, ypred))\n",
    "\n",
    "print(\"Min accuracy to beat for just random guessing in the TEST set:\")\n",
    "print(len(validation[validation.is_bad == 0])/len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
